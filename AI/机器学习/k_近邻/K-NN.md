K Nearest Neighbor算法又叫KNN算法，这个算法是机器学习里面一个比较经典的算法， 总体来说KNN算法是相对比较容易理解的算法

# 定义

如果一个样本在特征空间中的**k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别**，则该样本也属于这个类别。
即K-NN算法就是求点A在空间中K个最近的点，按照其类别进行投票，决定点A的类别。
# KNN算法流程

1）计算已知类别数据集中的点与当前点之间的距离
2）按距离递增次序排序
3）选取与当前点距离最小的k个点
4）统计前k个点所在的类别出现的频率
5）返回前k个点出现频率最高的类别作为当前点的预测分类
# k值的选择
- **K值过小**：
    - 容易受到异常点的影响
    - 容易过拟合
- **k值过大：**
    - 受到样本均衡的问题
    - 容易欠拟合

# K-NN算法优化
标准K-NN的算法的时间复杂度是：
$$
O(D*M*N)
$$
其中D是因为数据又D维，所以计算一次距离要计算D次，N是训练集，M是测试集，对一一个测试集数据，我们要计算$D*N$次距离，而对于M个参数的数据集，一共就需要计算如上所示的次数。
但是以上式子一般会写成：
$$
O(D*N^2)
$$
但是这个时间复杂度太高了，我们需要优化：
## 1、KD树
通过构造树和回调的方法，可以使时间复杂度降低到：
$$
O(D*\log(N)*M)
$$
具体做法是，选取一个点再选取一个维度，将训练集平分成两半，以这个点为树根构造树。搜索的时候就直接按照树的方式搜索。
寻到点之后画圆寻找交线，如果没有交线则直接选取最近点，如果有相切的线，就要进行回溯，确定最近的点。这就导致不需要遍历所有的的点就能获取最近的K个值。
## 2、BALL树
这个只知道比KD树优化效率还高，具体怎么实现没看