优化这里有两种方法（以后可能补其他的），这里对比一下

# 两种方法对比

|梯度下降|正规方程|
|---|---|
|需要选择学习率|不需要|
|需要迭代求解|一次运算得出|
|特征数量较大可以使用|需要计算方程，时间复杂度高O(n3)|

- 首先，最小二乘法需要计算$𝑋^𝑇𝑋$的逆矩阵，**有可能它的逆矩阵不存在**，这样就没有办法直接用最小二乘法了。
    - 此时就需要使用梯度下降法。当然，我们可以通过对样本数据进行整理，去掉冗余特征。让$𝑋^𝑇𝑋$的行列式不为0，然后继续使用最小二乘法。
- 第二，当样本特征n非常的大的时候，计算$𝑋^𝑇𝑋$的逆矩阵是一个非常耗时的工作（$n*n$的矩阵求逆），甚至不可行。
    - 此时以梯度下降为代表的迭代法仍然可以使用。
    - 那这个n到底多大就不适合最小二乘法呢？如果你没有很多的分布式大数据计算资源，建议超过10000个特征就用迭代法吧。或者通过主成分分析降低特征的维度后再用最小二乘法。
- 第三，如果拟合函数不是线性的，这时无法使用最小二乘法，需要通过一些技巧转化为线性才能使用，此时梯度下降仍然可以用。
- 第四，以下特殊情况，。
    - 当样本量m很少，小于特征数n的时候，这时拟合方程是欠定的，常用的优化方法都无法去拟合数据。
    - 当样本量m等于特征数n的时候，用方程组求解就可以了。
    - 当m大于n时，拟合方程是超定的，也就是我们常用与最小二乘法的场景了。
# 算法选择依据

- 小规模数据：
    - 正规方程：**LinearRegression(不能解决拟合问题)**
    - 岭回归
- 大规模数据：
    - 梯度下降法：**SGDRegressor**

经过前面介绍，我们发现在真正的开发中，我们使用梯度下降法偏多（深度学习中更加明显），下一节中我们会进一步介绍梯度下降法的一些原理。