梯度下降就是用求导数的方法逐步毕竟函数的最小值点。公式如下：
$$
\theta_{i+1}=\theta_i-\alpha\frac{\partial}{\partial\theta_i}J(\theta)\tag{1.1}
$$
其中$\theta$为参数，$\alpha$是学习率


梯度下降算法分为三类
1、**批量梯度下降法BGD**
2、**随机梯度下降法SGD**
3、**min-batch 小批量梯度下降法
## **三种梯度下降方法的总结**

1.批梯度下降每次更新使用了所有的训练数据，最小化损失函数，**如果只有一个极小值，那么批梯度下降是考虑了训练集所有数据，是朝着最小值迭代运动的，**但是缺点是如果样本值很大的话，更新速度会很慢。

2.随机梯度下降在每次更新的时候，只考虑了一个样本点，这样会大大加快训练数据，也恰好是批梯度下降的缺点，但是有可能由于训练数据的噪声点较多，**那么每一次利用噪声点进行更新的过程中，就不一定是朝着极小值方向更新，但是由于更新多轮，整体方向还是大致朝着极小值方向更新，又提高了速度。**

3.小批量梯度下降法是**为了解决批梯度下降法的训练速度慢，以及随机梯度下降法的准确性综合而来，但是这里注意，不同问题的batch是不一样的。