

评估语言模型性能的最佳方法是将其嵌入到应用程序中，并测量应用程序改进多少。这种端到端的评估称为外在评估。外在评估是了解语言模型（或任何组件）中的特定改进是否真的有助于手头任务的唯一方法。因此，为了评估作为语音识别或机器翻译等某些任务组成部分的 n-gram 语言模型，我们可以通过运行语音识别器或机器翻译器两次来比较两个候选语言模型的性能，每个语言模型一次，并查看哪个提供更准确的转录。不幸的是，端到端运行大型 NLP 系统通常非常昂贵。相反，拥有一个可用于快速评估语言模型中潜在改进的指标会很有帮助。内在评估指标是独立于任何应用程序来确保模型质量的指标。

所以我们需要以下指标：


1，[[困惑度]]