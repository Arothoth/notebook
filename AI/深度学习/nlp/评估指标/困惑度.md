困惑度指标是，称为 perplexity 的概率函数，是 NLP 中最重要的指标之一，用于评估大型语言模型以及 [[n-gram]] 模型。
它是指模型预测下一个单词时的概率，预测的概率越高困惑度越低。完美的语言模型的预测概率为1，困惑度为0.


# 计算：
语言模型在测试集上的困惑度（有时缩写为 PP 或 PPL）是测试集的逆概率，由单词（或标记）的数量标准化。因此，它有时被称为 per-word 或 per-token 困惑度。其计算公式如下：
$$
perplexity(W) = \sqrt[n]{\frac1{P(w_{1:n})}}\tag{1.1}
$$
对于<span style="font-weight:bold; color:rgb(0, 176, 240)">bigram（2-gram）</span>来说，为：
$$
perplexity(W) = \sqrt[n]{\prod_{i=1}^n\frac1{P(w_{i}|w_{i-1})}}\tag{1.2}
$$
# 平滑：

但是到现在有一个问题在于，训练集和测试即不一定完全一致，当测试集出现训练集完全没有出现过的组合时，困惑度无法计算。因为概率`P`为0，所以，我们就需要某种方法对其进行平滑处理，包括，拉普拉斯平滑，愚蠢的退缩`stupid backoff`, 和`n-gram`插值.