其中一般将词汇为表示成向量，称作词向量，再由各个词向量按顺序组成矩阵形成文本表示.

一般有三种方式

# 1、one_hot
举例如下：
```
["改变", "要", "如何", "起手"]
==> 
[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]
```
- 优势：操作简单，容易理解.
- 劣势：完全割裂了词与词之间的联系，而且在大语料集下，每个向量的长度过大，占据大量内存.


# 2、word2vec

将one_hot词汇表示成向量的无监督训练方法

有两种模式

## 1、CBOW(Continuous bag of words)

用多个词预测一个词。
假设窗口长为3，目标词向量为3，one-hot为5。
那么，用第一个和第三个词预测第二个。
第一个词和第三个词1\*5的one_hot 乘上3\*5的参数矩阵相加变成1\*3的向量。
然后这个1\*3的向量再去×5\*3的变换矩阵，然后和被预测的值计算损失反向传播。

最后使用1\*5的one_hot 乘上3\*5的参数矩阵相加变成1\*3的向量就是我们要的向量

## 2、skipgram
和CBOW反过来，用一个词预测多个词。


# 3、word embedding
也是把one_hot映射成词向量
词向量之间的距离可以体现词语之间的相关联程度

pytorch实现：[[torch.nn.Embedding]]
tensorflow实现：[[tf.keras.layers.Embedding]]
