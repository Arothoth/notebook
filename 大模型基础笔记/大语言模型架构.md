

# 1、能力拓展

模型的能力是在而是随着模型复杂度的提升凭空自然涌现的。这些能力因此被称为大语言模型的涌现能力（Emergent Abilities）。
而是随着模型复杂度的提升凭空自然涌现 5。这些能力因此被称为大语言模型的涌现能力（Emergent Abilities）。

# 2 主流模型架构

主要有：Encoder-only 架构，Decoder-only 架构以及 Encoder-Decoder 架构

其实可以理解为
Encoder 是理解
Decoder 是生成

## 2.1 BERT
BERT 是一种Encoder-only架构
### 2.1.1 BERT 预训练

先抽取原句中的样本句子
然后判断是否是链接在一起的两句话
然后遮掩部分token，进行预测
通过这种方法进行预训练，判断句子间的关系

### 2.1.2 BERT下游

BERT是Encoder 是理解模型，一般用于分类，理解微调

### 2.1.3 RoBERTa
其预训练改变了掩码方式：动态掩码语言建模。具体而言，BERT在数据预处理期间对句子进行掩码，随后在每个训练 epoch 中，掩码位置不再变化。而 RoBERTa 则将训练数据复制成 10 个副本，分别进行掩码。在同样训练 40个 epoch 的前提下，BERT 在其静态掩码后的文本上训练了 40 次，而 RoBERTa 将10 个不同掩码后的副本分别训练了 4 次，从而增加模型训练的多样性，有助于模型学习到更丰富的上下文信息。

