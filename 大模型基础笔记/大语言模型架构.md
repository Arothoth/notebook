

# 1、能力拓展

模型的能力是在而是随着模型复杂度的提升凭空自然涌现的。这些能力因此被称为大语言模型的涌现能力（Emergent Abilities）。而是随着模型复杂度的提升凭空自然涌现。这些能力因此被称为大语言模型的涌现能力（Emergent Abilities）。

# 2 主流模型架构

主要有：Encoder-only 架构，Decoder-only 架构以及 Encoder-Decoder 架构

其实可以理解为
Encoder 是理解
Decoder 是生成

## 2.1 BERT
BERT 是一种Encoder-only架构
### 2.1.1 BERT 预训练

先抽取原句中的样本句子
然后判断是否是链接在一起的两句话
然后遮掩部分token，进行预测
通过这种方法进行预训练，判断句子间的关系

### 2.1.2 BERT下游

BERT是Encoder 是理解模型，一般用于分类，理解微调

### 2.1.3 RoBERTa
其预训练改变了掩码方式：动态掩码语言建模。具体而言，BERT在数据预处理期间对句子进行掩码，随后在每个训练 epoch 中，掩码位置不再变化。而 RoBERTa 则将训练数据复制成 10 个副本，分别进行掩码。在同样训练 40个 epoch 的前提下，BERT 在其静态掩码后的文本上训练了 40 次，而 RoBERTa 将10 个不同掩码后的副本分别训练了 4 次，从而增加模型训练的多样性，有助于模型学习到更丰富的上下文信息。

### 2.1.4 ALBERT
**跨层参数共享：**
参数因子分解

在 BERT 中，Embedding 层的输出向量维度 E 与隐藏层的向量维度 H 是一致的，这意味着 Embedding 层的输出直接用作后续编码模块的输入。具体来说，BERT-Base 模型对应的词表大小 V 为 3,0000 左右，并且其隐藏层的向量维度 H 设置为 768。因此，BERT 的 Embedding 层需要的参数数量是 V × H，大约为 2,304万。

相比之下，ALBERT 将 Embedding 层的矩阵先进行分解，将词表对应的独热编码向量通过一个低维的投影层下投影至维度 E，再将其上投影回隐藏状态的维度 H。具体来说，ALBERT 选择了一个较小的 Embedding 层维度，例如 128，并将参数数量拆解为 V × E + E × H。按照这个设计，ALBERT 的 Embedding 层大约需要 394 万个参数，大约是 BERT 参数数量的六分之一。

**跨层参数共享**

以经典的 BERT-Base 模型为例，模型中共有 12 层相同架构的编码模块，所有Transformer 块的参数都是独立训练的。ALBERT 为了降低模型的参数量，提出了跨层参数共享机制，只学习第一层编码模块的参数，并将其直接共享给其他所有层。该机制在一定程度上牺牲了模型性能，但显著提升了参数存储空间的压缩比，从而实现了更高效的资源利用

预训练：
句序预测
即选择两句相邻的句子，然后将其拼接，让模型判断是否是正序还是倒叙

### 2.1.5 ELECTRA

**预训练：**

在模型结构上，ELECTRA 在 BERT 原有的掩码语言建模基础上结合了生成对抗网络（Generative Adversarial Network, GAN）的思想，采用了一种生成器-判别器结构。具体来说，ELECTRA 模型包含一个生成器和一个判别器，其中生成器（Generator）是一个能进行掩码预测的模型（例如 BERT 模型），负责将掩码后的文本恢复原状。而判别器（Discriminator）则使用替换词检测（Replaced TokenDetection, RTD）预训练任务，负责检测生成器输出的内容中的每个 Token 是否是原文中的内容。
![[Pasted image 20250726130558.png]]


## 2.2 Encoder to Decoder模型

解码器同样由多个解码模块堆叠而成，每个解码模块由一个带掩码的自注意力模块、一个交叉注意力模块和一个全连接前馈模块组成。

![[Pasted image 20250726134321.png]]


自注意模块在编码器和解码器中的注意力目标不同的。在编码器中，我们需要对输入序列的上下文进行“通盘考虑”，所以采用双向注意力机制以全面捕捉上下文信息。但在解码器中，自注意力机制则是单向的，仅以上文为条件来解码得到下文，通过掩码操作避免解码器“窥视”未来的信息。交叉注意力通过将解码器的查询（query）与编码器的键（key）和值（value）相结合，实现了两个模块间的有效信息交流。通过自注意力和交叉注意力机制的结合，Encoder-Decoder 架构能够高效地编码输入信息并生成高质量的输出序列。自注意力机制确保了输入序列和生成序列
内部的一致性和连贯性，而交叉注意力机制则确保了解码器在生成每个输出 Token时都能参考输入序列的全局上下文信息，从而生成与输入内容高度相关的结果。在这两个机制的共同作用下，Encoder-Decoder 架构不仅能够深入理解输入序列，还能够根据不同任务的需求灵活生成长度适宜的输出序列，在机器翻译、文本摘要、问答系统等任务中得到了广泛应用，并取得了显著成效。

### 2.2.1 T5

T5 模型的核心思想是将多种 NLP 任务统一到一个文本转文本的生成式框架中。在此统一框架下，T5 通过不同的输入前缀来指示模型执行不同任务，然后生成相应的任务输出，
#### 2.2.1.1 预训练
T5 提出了名为 Span Corruption 的预训练任务。这一预训练任务从原始输入中选择 15% 的 Token 进行破坏，每次都选择连续三个 Token 作为一个小段（span）整体被掩码成 [MASK]。与 BERT 模型中采用的单个 Token 预测不同，T5 模型需要对整个被遮挡的连续文本片段进行预测。这些片段可能包括连续的短语或子句，它们在自然语言中构成了具有完整意义的语义单元。
### 2.2.3 BART
在预训练任务上，BART 以重建被破坏的文本为目标。其通过 Token 遮挡任务（Token Masking）、Token 删除任务（Token Deletion）、连续文本填空任务（TextInfilling）、句子打乱任务（Sentence Permutation）以及文档旋转任务（Document Rotation）等五个任务来破坏文本，然后训练模型对原始文本进行恢复。这种方式锻炼了模型对文本结构和语义的深入理解，增强了其在面对不完整或损坏信息时的鲁棒性。五个文本破坏任务的具体形式如下所述。

Token 遮挡任务（Token Masking）：类似于 BERT 中的 MLM 任务，在原始文本中随机采样一部分 Token 并将其替换为 MASK，从而训练模型推断被删除的 Token 内容的能力。

Token 删除任务（Token Deletion）：在原始文本中随机删除一部分 Token，从而训练模型推断被删除的 Token 位置以及内容的能力。

连续文本填空任务（Text Infilling）：类似于 T5 的预训练任务，在原始文本中选择几段连续的 Token（每段作为一个 span），整体替换为 MASK。其中span 的长度服从 λ = 3 的泊松分布，如果长度为 0 则直接插入一个 MASK。这一任务旨在训练模型推断一段 span 及其长度的能力。

句子打乱任务（Sentence Permutation）：将给定文本拆分为多个句子，并随机打乱句子的顺序。旨在训练模型推理前后句关系的能力。

文档旋转任务（Document Rotation）：从给定文本中随机选取一个 Token，作为文本新的开头进行旋转。旨在训练模型找到文本合理起始点的能力。

##  2.3 Decoder-only

在开放式（Open-Ended）生成任务中，通常输入序列较为简单，甚至没有具体明确的输入，因此维持一个完整的编码器来处理这些输入并不是必要的。对于这种任务，Encoder-Decoder 架构可能显得过于复杂且缺乏灵活性。在这种背景下，Decoder-only 架构表现得更为优异。
它通过自回归方法逐字生成文本，不仅保持了长文本的连贯性和内在一致性，而且在缺乏明确输入或者复杂输入的情况下，能够更自然、流畅地生成文本。此外，Decoder-only 架构由于去除了编码器部分，使得模型更加轻量化，从而加快了训练和推理的速度。因此，在同样的模型规模下，Decoder-only 架构可能表现得更为出色。

### 2.3.1 gpt-1
![[Pasted image 20250726195040.png]]

掩码是在q和k的运算后乘上掩码，使其不能关注未来的位置

#### 2.3.1.1 预训练

GPT-1 使用小说数据集 BookCorpus来进行预训练，该数据集包含约 8 亿个 Token，总数据量接近 5GB。在预训练方法上，GPT-1 采用下一词预测任务，即基于给定的上文预测下一个可能出现的 Token。以自回归的方法不断完成下一词预测任务，模型可以有效地完成文本生成任务。通过这种预训练策略，模型可以在不需要人为构造大量带标签数据的前提下，学习到大量语言的“常识”，学会生成连贯且上下文相关的文本。这不仅提高了模型的泛化能力，而且减少了对标注数据的依赖，为自然语言处理领域带来了新的研究方向和应用前景。