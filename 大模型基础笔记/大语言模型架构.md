

# 1、能力拓展

模型的能力是在而是随着模型复杂度的提升凭空自然涌现的。这些能力因此被称为大语言模型的涌现能力（Emergent Abilities）。而是随着模型复杂度的提升凭空自然涌现。这些能力因此被称为大语言模型的涌现能力（Emergent Abilities）。

# 2 主流模型架构

主要有：Encoder-only 架构，Decoder-only 架构以及 Encoder-Decoder 架构

其实可以理解为
Encoder 是理解
Decoder 是生成

## 2.1 BERT
BERT 是一种Encoder-only架构
### 2.1.1 BERT 预训练

先抽取原句中的样本句子
然后判断是否是链接在一起的两句话
然后遮掩部分token，进行预测
通过这种方法进行预训练，判断句子间的关系

### 2.1.2 BERT下游

BERT是Encoder 是理解模型，一般用于分类，理解微调

### 2.1.3 RoBERTa
其预训练改变了掩码方式：动态掩码语言建模。具体而言，BERT在数据预处理期间对句子进行掩码，随后在每个训练 epoch 中，掩码位置不再变化。而 RoBERTa 则将训练数据复制成 10 个副本，分别进行掩码。在同样训练 40个 epoch 的前提下，BERT 在其静态掩码后的文本上训练了 40 次，而 RoBERTa 将10 个不同掩码后的副本分别训练了 4 次，从而增加模型训练的多样性，有助于模型学习到更丰富的上下文信息。

### 2.1.4 ALBERT
**跨层参数共享：**
参数因子分解

在 BERT 中，Embedding 层的输出向量维度 E 与隐藏层的向量维度 H 是一致的，这意味着 Embedding 层的输出直接用作后续编码模块的输入。具体来说，BERT-Base 模型对应的词表大小 V 为 3,0000 左右，并且其隐藏层的向量维度 H 设置为 768。因此，BERT 的 Embedding 层需要的参数数量是 V × H，大约为 2,304万。

相比之下，ALBERT 将 Embedding 层的矩阵先进行分解，将词表对应的独热编码向量通过一个低维的投影层下投影至维度 E，再将其上投影回隐藏状态的维度 H。具体来说，ALBERT 选择了一个较小的 Embedding 层维度，例如 128，并将参数数量拆解为 V × E + E × H。按照这个设计，ALBERT 的 Embedding 层大约需要 394 万个参数，大约是 BERT 参数数量的六分之一。

**跨层参数共享**

以经典的 BERT-Base 模型为例，模型中共有 12 层相同架构的编码模块，所有Transformer 块的参数都是独立训练的。ALBERT 为了降低模型的参数量，提出了跨层参数共享机制，只学习第一层编码模块的参数，并将其直接共享给其他所有层。该机制在一定程度上牺牲了模型性能，但显著提升了参数存储空间的压缩比，从而实现了更高效的资源利用

预训练：
句序预测
即选择两句相邻的句子，然后将其拼接，让模型判断是否是正序还是倒叙

### 2.1.5 ELECTRA

**预训练：**

在模型结构上，ELECTRA 在 BERT 原有的掩码语言建模基础上结合了生成对抗网络（Generative Adversarial Network, GAN）的思想，采用了一种生成器-判别器结构。具体来说，ELECTRA 模型包含一个生成器和一个判别器，其中生成器（Generator）是一个能进行掩码预测的模型（例如 BERT 模型），负责将掩码后的文本恢复原状。而判别器（Discriminator）则使用替换词检测（Replaced TokenDetection, RTD）预训练任务，负责检测生成器输出的内容中的每个 Token 是否是原文中的内容。
![[Pasted image 20250726130558.png]]


## 2.2 Encoder to Decoder模型

解码器同样由多个解码模块堆叠而成，每个解码模块由一个带掩码的自注意力模块、一个交叉注意力模块和一个全连接前馈模块组成。

![[Pasted image 20250726134321.png]]


自注意模块在编码器和解码器中的注意力目标不同的。在编码器中，我们需要对输入序列的上下文进行“通盘考虑”，所以采用双向注意力机制以全面捕捉上下文信息。但在解码器中，自注意力机制则是单向的，仅以上文为条件来解码得到下文，通过掩码操作避免解码器“窥视”未来的信息。交叉注意力通过将解码器的查询（query）与编码器的键（key）和值（value）相结合，实现了两个模块间的有效信息交流。通过自注意力和交叉注意力机制的结合，Encoder-Decoder 架构能够高效地编码输入信息并生成高质量的输出序列。自注意力机制确保了输入序列和生成序列
内部的一致性和连贯性，而交叉注意力机制则确保了解码器在生成每个输出 Token时都能参考输入序列的全局上下文信息，从而生成与输入内容高度相关的结果。在这两个机制的共同作用下，Encoder-Decoder 架构不仅能够深入理解输入序列，还能够根据不同任务的需求灵活生成长度适宜的输出序列，在机器翻译、文本摘要、问答系统等任务中得到了广泛应用，并取得了显著成效。

### 2.2.1 T5