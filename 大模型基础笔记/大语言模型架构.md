

# 1、能力拓展

模型的能力是在而是随着模型复杂度的提升凭空自然涌现的。这些能力因此被称为大语言模型的涌现能力（Emergent Abilities）。
而是随着模型复杂度的提升凭空自然涌现 5。这些能力因此被称为大语言模型的涌现能力（Emergent Abilities）。

# 2 主流模型架构

主要有：Encoder-only 架构，Decoder-only 架构以及 Encoder-Decoder 架构

其实可以理解为
Encoder 是理解
Decoder 是生成

## 2.1 BERT
BERT 是一种Encoder-only架构
### 2.1.1 BERT 预训练

先抽取原句中的样本句子
然后判断是否是链接在一起的两句话
然后遮掩部分token，进行预测
通过这种方法进行预训练，判断句子间的关系

### 2.1.2 BERT下游

BERT是Encoder 是理解模型，一般用于分类，理解微调

### 2.1.3 RoBERTa
其预训练改变了掩码方式：动态掩码语言建模。具体而言，BERT在数据预处理期间对句子进行掩码，随后在每个训练 epoch 中，掩码位置不再变化。而 RoBERTa 则将训练数据复制成 10 个副本，分别进行掩码。在同样训练 40个 epoch 的前提下，BERT 在其静态掩码后的文本上训练了 40 次，而 RoBERTa 将10 个不同掩码后的副本分别训练了 4 次，从而增加模型训练的多样性，有助于模型学习到更丰富的上下文信息。

### 2.1.4 ALBERT
**跨层参数共享：**
参数因子分解

在 BERT 中，Embedding 层的输出向量维度 E 与隐藏层的向量维度 H 是一致的，这意味着 Embedding 层的输出直接用作后续编码模块的输入。具体来说，BERT-Base 模型对应的词表大小 V 为 3,0000 左右，并且其隐藏层的向量维度 H 设置为 768。因此，BERT 的 Embedding 层需要的参数数量是 V × H，大约为 2,304万。

相比之下，ALBERT 将 Embedding 层的矩阵先进行分解，将词表对应的独热编码向量通过一个低维的投影层下投影至维度 E，再将其上投影回隐藏状态的维度 H。具体来说，ALBERT 选择了一个较小的 Embedding 层维度，例如 128，并将参数数量拆解为 V × E + E × H。按照这个设计，ALBERT 的 Embedding 层大约需要 394 万个参数，大约是 BERT 参数数量的六分之一。

**跨层参数共享**

以经典的 BERT-Base 模型为例，模型中共有 12 层相同架构的编码模块，所有Transformer 块的参数都是独立训练的。ALBERT 为了降低模型的参数量，提出了跨层参数共享机制，只学习第一层编码模块的参数，并将其直接共享给其他所有层。该机制在一定程度上牺牲了模型性能，但显著提升了参数存储空间的压缩比，从而实现了更高效的资源利用